{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b645c0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7546d335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the taxi zone geometry data\n",
    "geometry_df = pd.read_csv('taxi_zone_geometry.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(f\"Shape: {geometry_df.shape}\")\n",
    "print(f\"Columns: {list(geometry_df.columns)}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(geometry_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9baf123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 rows with duplicated zone_ids\n",
      "\n",
      "Duplicated entries:\n",
      "     zone_id                                      zone_name    borough\n",
      "57        56                                         Corona     Queens\n",
      "58        56                                         Corona     Queens\n",
      "192      103  Governor's Island/Ellis Island/Liberty Island  Manhattan\n",
      "193      103  Governor's Island/Ellis Island/Liberty Island  Manhattan\n",
      "194      103  Governor's Island/Ellis Island/Liberty Island  Manhattan\n",
      "\n",
      "Zone IDs with duplicates (count per ID):\n",
      "Zone ID 56: 2 occurrences\n",
      "  - Corona, Queens\n",
      "  - Corona, Queens\n",
      "\n",
      "Zone ID 103: 3 occurrences\n",
      "  - Governor's Island/Ellis Island/Liberty Island, Manhattan\n",
      "  - Governor's Island/Ellis Island/Liberty Island, Manhattan\n",
      "  - Governor's Island/Ellis Island/Liberty Island, Manhattan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find duplicated zone_ids\n",
    "duplicated_zone_ids = geometry_df[geometry_df.duplicated(subset=['zone_id'], keep=False)]\n",
    "\n",
    "if len(duplicated_zone_ids) > 0:\n",
    "    print(f\"Found {len(duplicated_zone_ids)} rows with duplicated zone_ids\")\n",
    "    print(\"\\nDuplicated entries:\")\n",
    "    print(duplicated_zone_ids[['zone_id', 'zone_name', 'borough']].sort_values('zone_id'))\n",
    "    \n",
    "    # Group by zone_id to see which specific IDs are duplicated\n",
    "    duplicate_groups = duplicated_zone_ids.groupby('zone_id').size().sort_index()\n",
    "    print(f\"\\nZone IDs with duplicates (count per ID):\")\n",
    "    for zone_id, count in duplicate_groups.items():\n",
    "        print(f\"Zone ID {zone_id}: {count} occurrences\")\n",
    "        zone_details = geometry_df[geometry_df['zone_id'] == zone_id][['zone_id', 'zone_name', 'borough']]\n",
    "        for _, row in zone_details.iterrows():\n",
    "            print(f\"  - {row['zone_name']}, {row['borough']}\")\n",
    "        print()\n",
    "        \n",
    "else:\n",
    "    print(\"No duplicated zone_ids found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a71ea8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique zone_ids: 260\n",
      "Total rows: 263\n",
      "Missing zone_ids: 0\n",
      "Zone ID range: 1 to 263\n",
      "\n",
      "Borough distribution:\n",
      "borough\n",
      "Bronx            43\n",
      "Brooklyn         61\n",
      "EWR               1\n",
      "Manhattan        69\n",
      "Queens           69\n",
      "Staten Island    20\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total unique zone_ids: {geometry_df['zone_id'].nunique()}\")\n",
    "print(f\"Total rows: {len(geometry_df)}\")\n",
    "print(f\"Missing zone_ids: {geometry_df['zone_id'].isna().sum()}\")\n",
    "\n",
    "# Show zone_id range\n",
    "print(f\"Zone ID range: {geometry_df['zone_id'].min()} to {geometry_df['zone_id'].max()}\")\n",
    "\n",
    "# Borough distribution\n",
    "print(f\"\\nBorough distribution:\")\n",
    "print(geometry_df['borough'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d75dab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: 263 rows\n",
      "After removing duplicates: 260 rows\n",
      "Removed 3 duplicate rows\n",
      "Remaining duplicates: 0\n",
      "\n",
      "Cleaned dataset saved as 'taxi_zone_preprocessed.csv'\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicated zone_id rows, keeping only the first occurrence\n",
    "geometry_df_cleaned = geometry_df.drop_duplicates(subset=['zone_id'], keep='first')\n",
    "\n",
    "print(f\"Original dataset: {len(geometry_df)} rows\")\n",
    "print(f\"After removing duplicates: {len(geometry_df_cleaned)} rows\")\n",
    "print(f\"Removed {len(geometry_df) - len(geometry_df_cleaned)} duplicate rows\")\n",
    "\n",
    "# Verify no duplicates remain\n",
    "remaining_duplicates = geometry_df_cleaned[geometry_df_cleaned.duplicated(subset=['zone_id'], keep=False)]\n",
    "print(f\"Remaining duplicates: {len(remaining_duplicates)}\")\n",
    "\n",
    "# Save the cleaned dataset\n",
    "geometry_df_cleaned.to_csv('taxi_zone_preprocessed.csv', index=False)\n",
    "print(\"\\nCleaned dataset saved as 'taxi_zone_preprocessed.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4279104",
   "metadata": {},
   "source": [
    "### yellow_taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42f7b704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 3,066,766\n",
      "Number of columns: 19\n",
      "Shape: (3066766, 19)\n",
      "\n",
      "Columns: ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'airport_fee']\n",
      "\n",
      "First 5 rows:\n",
      "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
      "0         2  2023-01-01 00:32:10   2023-01-01 00:40:36              1.0   \n",
      "1         2  2023-01-01 00:55:08   2023-01-01 01:01:27              1.0   \n",
      "2         2  2023-01-01 00:25:04   2023-01-01 00:37:49              1.0   \n",
      "3         1  2023-01-01 00:03:48   2023-01-01 00:13:25              0.0   \n",
      "4         2  2023-01-01 00:10:29   2023-01-01 00:21:19              1.0   \n",
      "\n",
      "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
      "0           0.97         1.0                  N           161           141   \n",
      "1           1.10         1.0                  N            43           237   \n",
      "2           2.51         1.0                  N            48           238   \n",
      "3           1.90         1.0                  N           138             7   \n",
      "4           1.43         1.0                  N           107            79   \n",
      "\n",
      "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
      "0             2          9.3   1.00      0.5        0.00           0.0   \n",
      "1             1          7.9   1.00      0.5        4.00           0.0   \n",
      "2             1         14.9   1.00      0.5       15.00           0.0   \n",
      "3             1         12.1   7.25      0.5        0.00           0.0   \n",
      "4             1         11.4   1.00      0.5        3.28           0.0   \n",
      "\n",
      "   improvement_surcharge  total_amount  congestion_surcharge  airport_fee  \n",
      "0                    1.0         14.30                   2.5         0.00  \n",
      "1                    1.0         16.90                   2.5         0.00  \n",
      "2                    1.0         34.90                   2.5         0.00  \n",
      "3                    1.0         20.85                   0.0         1.25  \n",
      "4                    1.0         19.68                   2.5         0.00  \n",
      "\n",
      "Parquet file info:\n",
      "File size: 442.66 MB\n",
      "Schema:\n",
      "VendorID: int64\n",
      "tpep_pickup_datetime: timestamp[us]\n",
      "tpep_dropoff_datetime: timestamp[us]\n",
      "passenger_count: double\n",
      "trip_distance: double\n",
      "RatecodeID: double\n",
      "store_and_fwd_flag: string\n",
      "PULocationID: int64\n",
      "DOLocationID: int64\n",
      "payment_type: int64\n",
      "fare_amount: double\n",
      "extra: double\n",
      "mta_tax: double\n",
      "tip_amount: double\n",
      "tolls_amount: double\n",
      "improvement_surcharge: double\n",
      "total_amount: double\n",
      "congestion_surcharge: double\n",
      "airport_fee: double\n",
      "-- schema metadata --\n",
      "pandas: '{\"index_columns\": [], \"column_indexes\": [], \"columns\": [{\"name\":' + 2492\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Read specific parquet file using PyArrow directly\n",
    "parquet_file = 'D:/tlc_yellow_taxi/2023/yellow_tripdata_2023-01.parquet'\n",
    "\n",
    "# Read the parquet file\n",
    "table = pq.read_table(parquet_file)\n",
    "taxi_df = table.to_pandas()\n",
    "\n",
    "print(f\"Number of records: {len(taxi_df):,}\")\n",
    "print(f\"Number of columns: {len(taxi_df.columns)}\")\n",
    "print(f\"Shape: {taxi_df.shape}\")\n",
    "\n",
    "print(f\"\\nColumns: {list(taxi_df.columns)}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(taxi_df.head())\n",
    "\n",
    "# Additional info about the parquet file\n",
    "print(f\"\\nParquet file info:\")\n",
    "print(f\"File size: {table.nbytes / 1024**2:.2f} MB\")\n",
    "print(f\"Schema:\")\n",
    "print(table.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e98e4ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INVALID LOCATION ID ANALYSIS\n",
      "============================================================\n",
      "Valid zone_id range: 1 to 263\n",
      "\n",
      "Invalid Records Summary:\n",
      "Records with invalid PULocationID: 41,763\n",
      "Records with invalid DOLocationID: 33,549\n",
      "Records with either invalid location: 58,248\n",
      "Percentage of invalid records: 1.90%\n",
      "\n",
      "--- Invalid PULocationID Details ---\n",
      "Unique invalid pickup location IDs: [264, 265]\n",
      "Count by invalid pickup location ID:\n",
      "  PULocationID 264: 40,116 records\n",
      "  PULocationID 265: 1,647 records\n",
      "\n",
      "--- Invalid DOLocationID Details ---\n",
      "Unique invalid dropoff location IDs: [264, 265]\n",
      "Count by invalid dropoff location ID:\n",
      "  DOLocationID 264: 22,591 records\n",
      "  DOLocationID 265: 10,958 records\n",
      "\n",
      "--- Sample Invalid Records ---\n",
      "    tpep_pickup_datetime  PULocationID  DOLocationID  trip_distance  \\\n",
      "11   2023-01-01 00:43:37            79           264           7.30   \n",
      "59   2023-01-01 00:22:39           132           265          16.02   \n",
      "98   2023-01-01 00:10:50           264           186           1.41   \n",
      "99   2023-01-01 00:27:35           264           114           1.70   \n",
      "100  2023-01-01 00:42:01           264            79           1.14   \n",
      "101  2023-01-01 00:53:36           264           239           6.19   \n",
      "115  2023-01-01 00:04:08           264           264           0.70   \n",
      "116  2023-01-01 00:21:30           264           137           1.10   \n",
      "224  2023-01-01 00:29:33           132           265          13.73   \n",
      "379  2023-01-01 00:58:15           264           263           2.48   \n",
      "\n",
      "     fare_amount  \n",
      "11          33.8  \n",
      "59          61.1  \n",
      "98          10.7  \n",
      "99          11.4  \n",
      "100          8.6  \n",
      "101         40.1  \n",
      "115          7.2  \n",
      "116          9.3  \n",
      "224         82.8  \n",
      "379         15.6  \n",
      "\n",
      "--- Null/Missing Location IDs ---\n",
      "Records with null PULocationID: 0\n",
      "Records with null DOLocationID: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for invalid location IDs in taxi data\n",
    "print(\"=\"*60)\n",
    "print(\"INVALID LOCATION ID ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define valid zone_id range\n",
    "min_zone_id = 1\n",
    "max_zone_id = 263\n",
    "print(f\"Valid zone_id range: {min_zone_id} to {max_zone_id}\")\n",
    "\n",
    "# Check for invalid PULocationID\n",
    "invalid_pickup = taxi_df[\n",
    "    (taxi_df['PULocationID'] < min_zone_id) | \n",
    "    (taxi_df['PULocationID'] > max_zone_id) |\n",
    "    (taxi_df['PULocationID'].isna())\n",
    "]\n",
    "\n",
    "# Check for invalid DOLocationID  \n",
    "invalid_dropoff = taxi_df[\n",
    "    (taxi_df['DOLocationID'] < min_zone_id) | \n",
    "    (taxi_df['DOLocationID'] > max_zone_id) |\n",
    "    (taxi_df['DOLocationID'].isna())\n",
    "]\n",
    "\n",
    "# Check for records with either invalid pickup OR dropoff\n",
    "invalid_either = taxi_df[\n",
    "    (taxi_df['PULocationID'] < min_zone_id) | \n",
    "    (taxi_df['PULocationID'] > max_zone_id) |\n",
    "    (taxi_df['PULocationID'].isna()) |\n",
    "    (taxi_df['DOLocationID'] < min_zone_id) | \n",
    "    (taxi_df['DOLocationID'] > max_zone_id) |\n",
    "    (taxi_df['DOLocationID'].isna())\n",
    "]\n",
    "\n",
    "print(f\"\\nInvalid Records Summary:\")\n",
    "print(f\"Records with invalid PULocationID: {len(invalid_pickup):,}\")\n",
    "print(f\"Records with invalid DOLocationID: {len(invalid_dropoff):,}\")\n",
    "print(f\"Records with either invalid location: {len(invalid_either):,}\")\n",
    "print(f\"Percentage of invalid records: {len(invalid_either)/len(taxi_df)*100:.2f}%\")\n",
    "\n",
    "# Show details of invalid pickup locations\n",
    "if len(invalid_pickup) > 0:\n",
    "    print(f\"\\n--- Invalid PULocationID Details ---\")\n",
    "    pickup_counts = invalid_pickup['PULocationID'].value_counts().sort_index()\n",
    "    print(f\"Unique invalid pickup location IDs: {pickup_counts.index.tolist()}\")\n",
    "    print(\"Count by invalid pickup location ID:\")\n",
    "    for loc_id, count in pickup_counts.head(10).items():\n",
    "        print(f\"  PULocationID {loc_id}: {count:,} records\")\n",
    "    if len(pickup_counts) > 10:\n",
    "        print(f\"  ... and {len(pickup_counts) - 10} more\")\n",
    "\n",
    "# Show details of invalid dropoff locations\n",
    "if len(invalid_dropoff) > 0:\n",
    "    print(f\"\\n--- Invalid DOLocationID Details ---\")\n",
    "    dropoff_counts = invalid_dropoff['DOLocationID'].value_counts().sort_index()\n",
    "    print(f\"Unique invalid dropoff location IDs: {dropoff_counts.index.tolist()}\")\n",
    "    print(\"Count by invalid dropoff location ID:\")\n",
    "    for loc_id, count in dropoff_counts.head(10).items():\n",
    "        print(f\"  DOLocationID {loc_id}: {count:,} records\")\n",
    "    if len(dropoff_counts) > 10:\n",
    "        print(f\"  ... and {len(dropoff_counts) - 10} more\")\n",
    "\n",
    "# Show sample records with invalid locations\n",
    "if len(invalid_either) > 0:\n",
    "    print(f\"\\n--- Sample Invalid Records ---\")\n",
    "    sample_cols = ['tpep_pickup_datetime', 'PULocationID', 'DOLocationID', 'trip_distance', 'fare_amount']\n",
    "    print(invalid_either[sample_cols].head(10))\n",
    "\n",
    "# Check for null values specifically\n",
    "null_pickup = taxi_df['PULocationID'].isna().sum()\n",
    "null_dropoff = taxi_df['DOLocationID'].isna().sum()\n",
    "\n",
    "print(f\"\\n--- Null/Missing Location IDs ---\")\n",
    "print(f\"Records with null PULocationID: {null_pickup:,}\")\n",
    "print(f\"Records with null DOLocationID: {null_dropoff:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec6ebf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE TAXI DATA ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "1. TEMPORAL ANALYSIS\n",
      "----------------------------------------\n",
      "Date range: 2008-12-31 23:01:42 to 2023-02-01 00:56:53\n",
      "Average trips per day: 85188\n",
      "Min daily trips: 2\n",
      "Max daily trips: 114877\n",
      "\n",
      "Peak hour: 18:00 with 215,889 trips\n",
      "Lowest hour: 4:00 with 17,835 trips\n",
      "\n",
      "2. TRIP CHARACTERISTICS\n",
      "----------------------------------------\n",
      "Trip Distance Statistics:\n",
      "  Mean: 3.85 miles\n",
      "  Median: 1.80 miles\n",
      "  Min: 0.00 miles\n",
      "  Max: 258928.15 miles\n",
      "  Zero distance trips: 45,862\n",
      "\n",
      "Trip Duration Statistics:\n",
      "  Mean: 15.67 minutes\n",
      "  Median: 11.52 minutes\n",
      "  Min: -29.20 minutes\n",
      "  Max: 10029.18 minutes\n",
      "  Very short trips (≤1 min): 33,616\n",
      "  Very long trips (≥3 hours): 3,048\n",
      "\n",
      "3. FARE ANALYSIS\n",
      "----------------------------------------\n",
      "fare_amount:\n",
      "  Mean: $18.37\n",
      "  Median: $12.80\n",
      "  Negative values: 25,049\n",
      "  Zero values: 1,110\n",
      "extra:\n",
      "  Mean: $1.54\n",
      "  Median: $1.00\n",
      "  Negative values: 12,407\n",
      "  Zero values: 1,240,718\n",
      "mta_tax:\n",
      "  Mean: $0.49\n",
      "  Median: $0.50\n",
      "  Negative values: 24,501\n",
      "  Zero values: 23,421\n",
      "tip_amount:\n",
      "  Mean: $3.37\n",
      "  Median: $2.72\n",
      "  Negative values: 225\n",
      "  Zero values: 694,757\n",
      "tolls_amount:\n",
      "  Mean: $0.52\n",
      "  Median: $0.00\n",
      "  Negative values: 1,377\n",
      "  Zero values: 2,840,307\n",
      "total_amount:\n",
      "  Mean: $27.02\n",
      "  Median: $20.16\n",
      "  Negative values: 25,204\n",
      "  Zero values: 568\n",
      "\n",
      "4. PAYMENT ANALYSIS\n",
      "----------------------------------------\n",
      "Payment Type Distribution:\n",
      "  Type 0: 71,743 (2.3%)\n",
      "  Type 1: 2,411,462 (78.6%)\n",
      "  Type 2: 532,241 (17.4%)\n",
      "  Type 3: 18,023 (0.6%)\n",
      "  Type 4: 33,297 (1.1%)\n",
      "\n",
      "5. PASSENGER ANALYSIS\n",
      "----------------------------------------\n",
      "Passenger Count Distribution:\n",
      "  0.0 passenger(s): 51,164 (1.7%)\n",
      "  1.0 passenger(s): 2,261,400 (73.7%)\n",
      "  2.0 passenger(s): 451,536 (14.7%)\n",
      "  3.0 passenger(s): 106,353 (3.5%)\n",
      "  4.0 passenger(s): 53,745 (1.8%)\n",
      "  5.0 passenger(s): 42,681 (1.4%)\n",
      "  6.0 passenger(s): 28,124 (0.9%)\n",
      "  7.0 passenger(s): 6 (0.0%)\n",
      "  8.0 passenger(s): 13 (0.0%)\n",
      "  9.0 passenger(s): 1 (0.0%)\n",
      "\n",
      "Unusual passenger counts:\n",
      "  Zero passengers: 51,164\n",
      "  More than 6 passengers: 20\n",
      "\n",
      "6. LOCATION ANALYSIS\n",
      "----------------------------------------\n",
      "Top 10 Pickup Locations:\n",
      "  1. Zone 132: 160,030 trips (5.2%)\n",
      "  2. Zone 237: 148,074 trips (4.8%)\n",
      "  3. Zone 236: 138,391 trips (4.5%)\n",
      "  4. Zone 161: 135,417 trips (4.4%)\n",
      "  5. Zone 186: 109,227 trips (3.6%)\n",
      "  6. Zone 162: 105,334 trips (3.4%)\n",
      "  7. Zone 142: 100,228 trips (3.3%)\n",
      "  8. Zone 230: 98,991 trips (3.2%)\n",
      "  9. Zone 138: 89,188 trips (2.9%)\n",
      "  10. Zone 170: 88,346 trips (2.9%)\n",
      "\n",
      "Top 10 Dropoff Locations:\n",
      "  1. Zone 236: 146,348 trips (4.8%)\n",
      "  2. Zone 237: 132,364 trips (4.3%)\n",
      "  3. Zone 161: 116,149 trips (3.8%)\n",
      "  4. Zone 230: 89,878 trips (2.9%)\n",
      "  5. Zone 170: 88,783 trips (2.9%)\n",
      "  6. Zone 239: 87,969 trips (2.9%)\n",
      "  7. Zone 142: 87,969 trips (2.9%)\n",
      "  8. Zone 141: 87,655 trips (2.9%)\n",
      "  9. Zone 162: 82,739 trips (2.7%)\n",
      "  10. Zone 48: 77,383 trips (2.5%)\n",
      "\n",
      "7. DATA QUALITY ISSUES\n",
      "----------------------------------------\n",
      "Missing Values by Column:\n",
      "  passenger_count: 71,743 (2.34%)\n",
      "  RatecodeID: 71,743 (2.34%)\n",
      "  store_and_fwd_flag: 71,743 (2.34%)\n",
      "  congestion_surcharge: 71,743 (2.34%)\n",
      "  airport_fee: 71,743 (2.34%)\n",
      "\n",
      "Logical Issues:\n",
      "  Pickup after dropoff: 3\n",
      "  Negative trip distances: 0\n",
      "  Negative durations: 3\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Further Data Analysis\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE TAXI DATA ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. TEMPORAL ANALYSIS\n",
    "print(\"\\n1. TEMPORAL ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Convert datetime columns\n",
    "taxi_df['tpep_pickup_datetime'] = pd.to_datetime(taxi_df['tpep_pickup_datetime'])\n",
    "taxi_df['tpep_dropoff_datetime'] = pd.to_datetime(taxi_df['tpep_dropoff_datetime'])\n",
    "\n",
    "# Calculate trip duration\n",
    "taxi_df['trip_duration_minutes'] = (taxi_df['tpep_dropoff_datetime'] - taxi_df['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "\n",
    "print(f\"Date range: {taxi_df['tpep_pickup_datetime'].min()} to {taxi_df['tpep_pickup_datetime'].max()}\")\n",
    "\n",
    "# Daily trip counts\n",
    "daily_trips = taxi_df['tpep_pickup_datetime'].dt.date.value_counts().sort_index()\n",
    "print(f\"Average trips per day: {daily_trips.mean():.0f}\")\n",
    "print(f\"Min daily trips: {daily_trips.min()}\")\n",
    "print(f\"Max daily trips: {daily_trips.max()}\")\n",
    "\n",
    "# Hourly distribution\n",
    "hourly_trips = taxi_df['tpep_pickup_datetime'].dt.hour.value_counts().sort_index()\n",
    "print(f\"\\nPeak hour: {hourly_trips.idxmax()}:00 with {hourly_trips.max():,} trips\")\n",
    "print(f\"Lowest hour: {hourly_trips.idxmin()}:00 with {hourly_trips.min():,} trips\")\n",
    "\n",
    "# 2. TRIP CHARACTERISTICS\n",
    "print(\"\\n2. TRIP CHARACTERISTICS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Trip distance analysis\n",
    "print(\"Trip Distance Statistics:\")\n",
    "print(f\"  Mean: {taxi_df['trip_distance'].mean():.2f} miles\")\n",
    "print(f\"  Median: {taxi_df['trip_distance'].median():.2f} miles\")\n",
    "print(f\"  Min: {taxi_df['trip_distance'].min():.2f} miles\")\n",
    "print(f\"  Max: {taxi_df['trip_distance'].max():.2f} miles\")\n",
    "print(f\"  Zero distance trips: {(taxi_df['trip_distance'] == 0).sum():,}\")\n",
    "\n",
    "# Trip duration analysis\n",
    "print(\"\\nTrip Duration Statistics:\")\n",
    "print(f\"  Mean: {taxi_df['trip_duration_minutes'].mean():.2f} minutes\")\n",
    "print(f\"  Median: {taxi_df['trip_duration_minutes'].median():.2f} minutes\")\n",
    "print(f\"  Min: {taxi_df['trip_duration_minutes'].min():.2f} minutes\")\n",
    "print(f\"  Max: {taxi_df['trip_duration_minutes'].max():.2f} minutes\")\n",
    "\n",
    "# Identify unusual trips\n",
    "short_trips = taxi_df[taxi_df['trip_duration_minutes'] <= 1]\n",
    "long_trips = taxi_df[taxi_df['trip_duration_minutes'] >= 180]  # 3+ hours\n",
    "print(f\"  Very short trips (≤1 min): {len(short_trips):,}\")\n",
    "print(f\"  Very long trips (≥3 hours): {len(long_trips):,}\")\n",
    "\n",
    "# 3. FARE ANALYSIS\n",
    "print(\"\\n3. FARE ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "fare_cols = ['fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'total_amount']\n",
    "for col in fare_cols:\n",
    "    if col in taxi_df.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(f\"  Mean: ${taxi_df[col].mean():.2f}\")\n",
    "        print(f\"  Median: ${taxi_df[col].median():.2f}\")\n",
    "        print(f\"  Negative values: {(taxi_df[col] < 0).sum():,}\")\n",
    "        print(f\"  Zero values: {(taxi_df[col] == 0).sum():,}\")\n",
    "\n",
    "# 4. PAYMENT ANALYSIS\n",
    "print(\"\\n4. PAYMENT ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'payment_type' in taxi_df.columns:\n",
    "    payment_counts = taxi_df['payment_type'].value_counts().sort_index()\n",
    "    print(\"Payment Type Distribution:\")\n",
    "    for payment_type, count in payment_counts.items():\n",
    "        percentage = (count / len(taxi_df)) * 100\n",
    "        print(f\"  Type {payment_type}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# 5. PASSENGER ANALYSIS\n",
    "print(\"\\n5. PASSENGER ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'passenger_count' in taxi_df.columns:\n",
    "    passenger_counts = taxi_df['passenger_count'].value_counts().sort_index()\n",
    "    print(\"Passenger Count Distribution:\")\n",
    "    for passengers, count in passenger_counts.items():\n",
    "        percentage = (count / len(taxi_df)) * 100\n",
    "        print(f\"  {passengers} passenger(s): {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Check for unusual passenger counts\n",
    "    zero_passengers = (taxi_df['passenger_count'] == 0).sum()\n",
    "    high_passengers = (taxi_df['passenger_count'] > 6).sum()\n",
    "    print(f\"\\nUnusual passenger counts:\")\n",
    "    print(f\"  Zero passengers: {zero_passengers:,}\")\n",
    "    print(f\"  More than 6 passengers: {high_passengers:,}\")\n",
    "\n",
    "# 6. LOCATION ANALYSIS\n",
    "print(\"\\n6. LOCATION ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Most popular pickup locations\n",
    "top_pickup = taxi_df['PULocationID'].value_counts().head(10)\n",
    "print(\"Top 10 Pickup Locations:\")\n",
    "for i, (location_id, count) in enumerate(top_pickup.items(), 1):\n",
    "    percentage = (count / len(taxi_df)) * 100\n",
    "    print(f\"  {i}. Zone {location_id}: {count:,} trips ({percentage:.1f}%)\")\n",
    "\n",
    "# Most popular dropoff locations  \n",
    "top_dropoff = taxi_df['DOLocationID'].value_counts().head(10)\n",
    "print(\"\\nTop 10 Dropoff Locations:\")\n",
    "for i, (location_id, count) in enumerate(top_dropoff.items(), 1):\n",
    "    percentage = (count / len(taxi_df)) * 100\n",
    "    print(f\"  {i}. Zone {location_id}: {count:,} trips ({percentage:.1f}%)\")\n",
    "\n",
    "# 7. DATA QUALITY ISSUES\n",
    "print(\"\\n7. DATA QUALITY ISSUES\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing Values by Column:\")\n",
    "missing_data = taxi_df.isnull().sum()\n",
    "for col, missing_count in missing_data.items():\n",
    "    if missing_count > 0:\n",
    "        percentage = (missing_count / len(taxi_df)) * 100\n",
    "        print(f\"  {col}: {missing_count:,} ({percentage:.2f}%)\")\n",
    "\n",
    "# Check for logical inconsistencies\n",
    "pickup_after_dropoff = taxi_df[taxi_df['tpep_pickup_datetime'] > taxi_df['tpep_dropoff_datetime']]\n",
    "print(f\"\\nLogical Issues:\")\n",
    "print(f\"  Pickup after dropoff: {len(pickup_after_dropoff):,}\")\n",
    "print(f\"  Negative trip distances: {(taxi_df['trip_distance'] < 0).sum():,}\")\n",
    "print(f\"  Negative durations: {(taxi_df['trip_duration_minutes'] < 0).sum():,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3019e08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEMPORAL DATA QUALITY ANALYSIS - NON-JANUARY 2023 RECORDS\n",
      "======================================================================\n",
      "Total records: 3,066,766\n",
      "\n",
      "Records with pickup time NOT in January 2023: 48\n",
      "Records with dropoff time NOT in January 2023: 642\n",
      "Records with either pickup OR dropoff NOT in January 2023: 655\n",
      "Percentage of records with temporal issues: 0.02%\n",
      "\n",
      "--- Pickup Year-Month Distribution (Non-January 2023) ---\n",
      "Top 10 pickup periods:\n",
      "  2008-12: 2 records\n",
      "  2022-10: 11 records\n",
      "  2022-12: 25 records\n",
      "  2023-02: 10 records\n",
      "\n",
      "Pickup date range for non-January 2023 records:\n",
      "  Earliest: 2008-12-31 23:01:42\n",
      "  Latest: 2023-02-01 00:56:53\n",
      "\n",
      "--- Dropoff Year-Month Distribution (Non-January 2023) ---\n",
      "Top 10 dropoff periods:\n",
      "  2009-01: 2 records\n",
      "  2022-10: 11 records\n",
      "  2022-12: 12 records\n",
      "  2023-02: 617 records\n",
      "\n",
      "Dropoff date range for non-January 2023 records:\n",
      "  Earliest: 2009-01-01 14:29:11\n",
      "  Latest: 2023-02-02 09:28:47\n",
      "\n",
      "--- Sample Records with Temporal Issues ---\n",
      "     tpep_pickup_datetime tpep_dropoff_datetime pickup_year_month  \\\n",
      "80    2022-12-31 23:51:30   2022-12-31 23:56:06           2022-12   \n",
      "567   2022-12-31 23:59:37   2023-01-01 00:07:28           2022-12   \n",
      "761   2022-12-31 23:58:27   2023-01-01 00:02:21           2022-12   \n",
      "900   2022-12-31 23:44:31   2022-12-31 23:48:47           2022-12   \n",
      "1177  2022-12-31 23:59:25   2023-01-01 00:19:21           2022-12   \n",
      "2172  2022-12-31 14:24:54   2022-12-31 14:35:25           2022-12   \n",
      "2173  2022-12-31 14:39:43   2022-12-31 14:43:37           2022-12   \n",
      "2588  2022-12-31 23:58:08   2023-01-01 00:08:28           2022-12   \n",
      "2696  2022-12-31 23:58:43   2023-01-01 00:09:39           2022-12   \n",
      "2931  2022-12-31 22:02:51   2022-12-31 22:28:31           2022-12   \n",
      "\n",
      "     dropoff_year_month  trip_distance  fare_amount  \n",
      "80              2022-12           0.86          6.5  \n",
      "567             2023-01           2.38         12.1  \n",
      "761             2023-01           0.49          5.8  \n",
      "900             2022-12           0.52          5.8  \n",
      "1177            2023-01           3.65         19.1  \n",
      "2172            2022-12           0.41         10.0  \n",
      "2173            2022-12           0.54          5.8  \n",
      "2588            2023-01           2.83         14.2  \n",
      "2696            2023-01           2.11         12.8  \n",
      "2931            2022-12           7.78         35.9  \n",
      "\n",
      "--- Specific Temporal Anomalies ---\n",
      "Records with pickup in other months of 2023: 10\n",
      "Records with dropoff in other months of 2023: 617\n",
      "Records with pickup in other years: 38\n",
      "Records with dropoff in other years: 25\n",
      "\n",
      "Trips that span across different months: 622\n",
      "Sample cross-month trips:\n",
      "     tpep_pickup_datetime tpep_dropoff_datetime pickup_year_month  \\\n",
      "567   2022-12-31 23:59:37   2023-01-01 00:07:28           2022-12   \n",
      "761   2022-12-31 23:58:27   2023-01-01 00:02:21           2022-12   \n",
      "1177  2022-12-31 23:59:25   2023-01-01 00:19:21           2022-12   \n",
      "2588  2022-12-31 23:58:08   2023-01-01 00:08:28           2022-12   \n",
      "2696  2022-12-31 23:58:43   2023-01-01 00:09:39           2022-12   \n",
      "\n",
      "     dropoff_year_month  \n",
      "567             2023-01  \n",
      "761             2023-01  \n",
      "1177            2023-01  \n",
      "2588            2023-01  \n",
      "2696            2023-01  \n",
      "\n",
      "--- January 2023 Data Summary ---\n",
      "Records with both pickup and dropoff in January 2023: 3,066,111\n",
      "Percentage of valid January 2023 records: 99.98%\n",
      "\n",
      "January 2023 date range:\n",
      "  Earliest pickup: 2023-01-01 00:00:00\n",
      "  Latest pickup: 2023-01-31 23:58:56\n",
      "  Earliest dropoff: 2023-01-01 00:03:28\n",
      "  Latest dropoff: 2023-01-31 23:59:59\n",
      "\n",
      "======================================================================\n",
      "JANUARY 2023 TEMPORAL ANALYSIS COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Analyze records with pickup/dropoff times not in January 2023\n",
    "print(\"=\"*70)\n",
    "print(\"TEMPORAL DATA QUALITY ANALYSIS - NON-JANUARY 2023 RECORDS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Convert datetime columns if not already done\n",
    "taxi_df['tpep_pickup_datetime'] = pd.to_datetime(taxi_df['tpep_pickup_datetime'])\n",
    "taxi_df['tpep_dropoff_datetime'] = pd.to_datetime(taxi_df['tpep_dropoff_datetime'])\n",
    "\n",
    "# Extract year and month from pickup and dropoff datetime\n",
    "taxi_df['pickup_year'] = taxi_df['tpep_pickup_datetime'].dt.year\n",
    "taxi_df['pickup_month'] = taxi_df['tpep_pickup_datetime'].dt.month\n",
    "taxi_df['dropoff_year'] = taxi_df['tpep_dropoff_datetime'].dt.year\n",
    "taxi_df['dropoff_month'] = taxi_df['tpep_dropoff_datetime'].dt.month\n",
    "\n",
    "# Create year-month combinations for easier analysis\n",
    "taxi_df['pickup_year_month'] = taxi_df['tpep_pickup_datetime'].dt.to_period('M')\n",
    "taxi_df['dropoff_year_month'] = taxi_df['tpep_dropoff_datetime'].dt.to_period('M')\n",
    "\n",
    "# Find records with pickup time not in January 2023\n",
    "pickup_not_jan_2023 = taxi_df[\n",
    "    (taxi_df['pickup_year'] != 2023) | \n",
    "    (taxi_df['pickup_month'] != 1)\n",
    "]\n",
    "\n",
    "# Find records with dropoff time not in January 2023\n",
    "dropoff_not_jan_2023 = taxi_df[\n",
    "    (taxi_df['dropoff_year'] != 2023) | \n",
    "    (taxi_df['dropoff_month'] != 1)\n",
    "]\n",
    "\n",
    "# Find records with either pickup OR dropoff not in January 2023\n",
    "either_not_jan_2023 = taxi_df[\n",
    "    (taxi_df['pickup_year'] != 2023) | \n",
    "    (taxi_df['pickup_month'] != 1) |\n",
    "    (taxi_df['dropoff_year'] != 2023) | \n",
    "    (taxi_df['dropoff_month'] != 1)\n",
    "]\n",
    "\n",
    "print(f\"Total records: {len(taxi_df):,}\")\n",
    "print(f\"\\nRecords with pickup time NOT in January 2023: {len(pickup_not_jan_2023):,}\")\n",
    "print(f\"Records with dropoff time NOT in January 2023: {len(dropoff_not_jan_2023):,}\")\n",
    "print(f\"Records with either pickup OR dropoff NOT in January 2023: {len(either_not_jan_2023):,}\")\n",
    "print(f\"Percentage of records with temporal issues: {len(either_not_jan_2023)/len(taxi_df)*100:.2f}%\")\n",
    "\n",
    "# Analyze pickup year-month distribution for non-January 2023\n",
    "if len(pickup_not_jan_2023) > 0:\n",
    "    print(f\"\\n--- Pickup Year-Month Distribution (Non-January 2023) ---\")\n",
    "    pickup_period_counts = pickup_not_jan_2023['pickup_year_month'].value_counts().sort_index()\n",
    "    print(\"Top 10 pickup periods:\")\n",
    "    for period, count in pickup_period_counts.head(10).items():\n",
    "        print(f\"  {period}: {count:,} records\")\n",
    "    \n",
    "    if len(pickup_period_counts) > 10:\n",
    "        print(f\"  ... and {len(pickup_period_counts) - 10} more periods\")\n",
    "    \n",
    "    # Show date range for non-January 2023 pickups\n",
    "    print(f\"\\nPickup date range for non-January 2023 records:\")\n",
    "    print(f\"  Earliest: {pickup_not_jan_2023['tpep_pickup_datetime'].min()}\")\n",
    "    print(f\"  Latest: {pickup_not_jan_2023['tpep_pickup_datetime'].max()}\")\n",
    "\n",
    "# Analyze dropoff year-month distribution for non-January 2023\n",
    "if len(dropoff_not_jan_2023) > 0:\n",
    "    print(f\"\\n--- Dropoff Year-Month Distribution (Non-January 2023) ---\")\n",
    "    dropoff_period_counts = dropoff_not_jan_2023['dropoff_year_month'].value_counts().sort_index()\n",
    "    print(\"Top 10 dropoff periods:\")\n",
    "    for period, count in dropoff_period_counts.head(10).items():\n",
    "        print(f\"  {period}: {count:,} records\")\n",
    "    \n",
    "    if len(dropoff_period_counts) > 10:\n",
    "        print(f\"  ... and {len(dropoff_period_counts) - 10} more periods\")\n",
    "    \n",
    "    # Show date range for non-January 2023 dropoffs\n",
    "    print(f\"\\nDropoff date range for non-January 2023 records:\")\n",
    "    print(f\"  Earliest: {dropoff_not_jan_2023['tpep_dropoff_datetime'].min()}\")\n",
    "    print(f\"  Latest: {dropoff_not_jan_2023['tpep_dropoff_datetime'].max()}\")\n",
    "\n",
    "# Show sample records with temporal issues\n",
    "if len(either_not_jan_2023) > 0:\n",
    "    print(f\"\\n--- Sample Records with Temporal Issues ---\")\n",
    "    sample_cols = ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'pickup_year_month', 'dropoff_year_month', 'trip_distance', 'fare_amount']\n",
    "    print(either_not_jan_2023[sample_cols].head(10))\n",
    "\n",
    "# Check for specific temporal anomalies\n",
    "print(f\"\\n--- Specific Temporal Anomalies ---\")\n",
    "\n",
    "# Records from other months in 2023\n",
    "other_months_2023_pickup = taxi_df[\n",
    "    (taxi_df['pickup_year'] == 2023) & \n",
    "    (taxi_df['pickup_month'] != 1)\n",
    "]\n",
    "\n",
    "other_months_2023_dropoff = taxi_df[\n",
    "    (taxi_df['dropoff_year'] == 2023) & \n",
    "    (taxi_df['dropoff_month'] != 1)\n",
    "]\n",
    "\n",
    "# Records from other years\n",
    "other_years_pickup = taxi_df[taxi_df['pickup_year'] != 2023]\n",
    "other_years_dropoff = taxi_df[taxi_df['dropoff_year'] != 2023]\n",
    "\n",
    "print(f\"Records with pickup in other months of 2023: {len(other_months_2023_pickup):,}\")\n",
    "print(f\"Records with dropoff in other months of 2023: {len(other_months_2023_dropoff):,}\")\n",
    "print(f\"Records with pickup in other years: {len(other_years_pickup):,}\")\n",
    "print(f\"Records with dropoff in other years: {len(other_years_dropoff):,}\")\n",
    "\n",
    "# Check for records that span across months\n",
    "cross_month_trips = taxi_df[\n",
    "    (taxi_df['pickup_year_month'] != taxi_df['dropoff_year_month'])\n",
    "]\n",
    "\n",
    "print(f\"\\nTrips that span across different months: {len(cross_month_trips):,}\")\n",
    "\n",
    "if len(cross_month_trips) > 0:\n",
    "    print(\"Sample cross-month trips:\")\n",
    "    cross_sample = cross_month_trips[['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'pickup_year_month', 'dropoff_year_month']].head(5)\n",
    "    print(cross_sample)\n",
    "\n",
    "# Summary of January 2023 data\n",
    "records_jan_2023 = taxi_df[\n",
    "    (taxi_df['pickup_year'] == 2023) & \n",
    "    (taxi_df['pickup_month'] == 1) &\n",
    "    (taxi_df['dropoff_year'] == 2023) & \n",
    "    (taxi_df['dropoff_month'] == 1)\n",
    "]\n",
    "\n",
    "print(f\"\\n--- January 2023 Data Summary ---\")\n",
    "print(f\"Records with both pickup and dropoff in January 2023: {len(records_jan_2023):,}\")\n",
    "print(f\"Percentage of valid January 2023 records: {len(records_jan_2023)/len(taxi_df)*100:.2f}%\")\n",
    "\n",
    "# Additional January 2023 analysis\n",
    "if len(records_jan_2023) > 0:\n",
    "    print(f\"\\nJanuary 2023 date range:\")\n",
    "    print(f\"  Earliest pickup: {records_jan_2023['tpep_pickup_datetime'].min()}\")\n",
    "    print(f\"  Latest pickup: {records_jan_2023['tpep_pickup_datetime'].max()}\")\n",
    "    print(f\"  Earliest dropoff: {records_jan_2023['tpep_dropoff_datetime'].min()}\")\n",
    "    print(f\"  Latest dropoff: {records_jan_2023['tpep_dropoff_datetime'].max()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"JANUARY 2023 TEMPORAL ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d0a305d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TAXI DATA CLEANING PROCESS\n",
      "================================================================================\n",
      "Original dataset size: 3,066,766 records\n",
      "\n",
      "1. REMOVING RECORDS NOT IN JANUARY 2023\n",
      "--------------------------------------------------\n",
      "Records NOT in January 2023: 655\n",
      "Remaining records: 3,066,111\n",
      "\n",
      "2. REMOVING RECORDS WITH INVALID LOCATION IDs\n",
      "--------------------------------------------------\n",
      "Records with invalid location IDs: 58,218\n",
      "Remaining records: 3,007,893\n",
      "\n",
      "3. REMOVING ZERO DISTANCE TRIPS\n",
      "--------------------------------------------------\n",
      "Records with zero distance: 41,520\n",
      "Remaining records: 2,966,373\n",
      "\n",
      "4. REMOVING TRIPS WITH NEGATIVE DURATION\n",
      "--------------------------------------------------\n",
      "Records with negative duration: 56\n",
      "Remaining records: 2,966,317\n",
      "\n",
      "5. REMOVING RECORDS WITH NEGATIVE FARE VALUES\n",
      "--------------------------------------------------\n",
      "  fare_amount: 21,303 negative values\n",
      "  extra: 10,822 negative values\n",
      "  mta_tax: 21,153 negative values\n",
      "  tip_amount: 30 negative values\n",
      "  tolls_amount: 1,172 negative values\n",
      "  total_amount: 21,404 negative values\n",
      "Total records with negative fare values: 21,413\n",
      "Remaining records: 2,944,904\n",
      "\n",
      "6. REMOVING RECORDS WITH MISSING VALUES\n",
      "--------------------------------------------------\n",
      "Total records with missing critical values: 0\n",
      "Remaining records: 2,944,904\n",
      "\n",
      "7. REMOVING TRIPS WITH 0 PASSENGERS\n",
      "--------------------------------------------------\n",
      "Records with 0 passengers: 112,707\n",
      "Remaining records: 2,832,197\n",
      "\n",
      "8. ADDITIONAL DATA QUALITY CHECKS\n",
      "--------------------------------------------------\n",
      "Records with trips longer than 24 hours: 16\n",
      "Remaining records: 2,832,181\n",
      "Records with trips longer than 100 miles: 10\n",
      "Remaining records: 2,832,171\n",
      "\n",
      "================================================================================\n",
      "DATA CLEANING SUMMARY\n",
      "================================================================================\n",
      "Original dataset: 3,066,766 records\n",
      "Final cleaned dataset: 2,832,171 records\n",
      "Total records removed: 234,595\n",
      "Percentage of data retained: 92.35%\n",
      "\n",
      "Cleaning steps breakdown:\n",
      "  1. Non-January 2023 records: 655\n",
      "  2. Invalid location IDs: 58,218\n",
      "  3. Zero distance trips: 41,520\n",
      "  4. Negative duration trips: 56\n",
      "  5. Negative fare values: 21,413\n",
      "  6. Missing critical values: 0\n",
      "  7. Zero passengers: 112,707\n",
      "  8. Trips > 24 hours: 16\n",
      "  9. Trips > 100 miles: 10\n",
      "\n",
      "Cleaned dataset info:\n",
      "Shape: (2832171, 26)\n",
      "Date range: 2023-01-01 00:00:05 to 2023-01-31 23:56:19\n",
      "Trip distance range: 0.01 to 92.60 miles\n",
      "Trip duration range: 0.02 to 1439.80 minutes\n",
      "\n",
      "================================================================================\n",
      "DATA CLEANING COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Data Cleaning Process\n",
    "print(\"=\"*80)\n",
    "print(\"TAXI DATA CLEANING PROCESS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Start with original dataset\n",
    "print(f\"Original dataset size: {len(taxi_df):,} records\")\n",
    "\n",
    "# Keep track of cleaning steps\n",
    "cleaning_steps = []\n",
    "\n",
    "# 1. Remove records with pickup/dropoff times NOT in January 2023\n",
    "print(\"\\n1. REMOVING RECORDS NOT IN JANUARY 2023\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Ensure datetime columns are properly converted\n",
    "taxi_df['tpep_pickup_datetime'] = pd.to_datetime(taxi_df['tpep_pickup_datetime'])\n",
    "taxi_df['tpep_dropoff_datetime'] = pd.to_datetime(taxi_df['tpep_dropoff_datetime'])\n",
    "\n",
    "# Extract year and month\n",
    "taxi_df['pickup_year'] = taxi_df['tpep_pickup_datetime'].dt.year\n",
    "taxi_df['pickup_month'] = taxi_df['tpep_pickup_datetime'].dt.month\n",
    "taxi_df['dropoff_year'] = taxi_df['tpep_dropoff_datetime'].dt.year\n",
    "taxi_df['dropoff_month'] = taxi_df['tpep_dropoff_datetime'].dt.month\n",
    "\n",
    "# Create mask for January 2023 records (both pickup AND dropoff must be in Jan 2023)\n",
    "jan_2023_mask = (\n",
    "    (taxi_df['pickup_year'] == 2023) & \n",
    "    (taxi_df['pickup_month'] == 1) &\n",
    "    (taxi_df['dropoff_year'] == 2023) & \n",
    "    (taxi_df['dropoff_month'] == 1)\n",
    ")\n",
    "\n",
    "non_jan_2023_count = len(taxi_df) - jan_2023_mask.sum()\n",
    "print(f\"Records NOT in January 2023: {non_jan_2023_count:,}\")\n",
    "\n",
    "taxi_df_clean = taxi_df[jan_2023_mask].copy()\n",
    "cleaning_steps.append(f\"Non-January 2023 records: {non_jan_2023_count:,}\")\n",
    "print(f\"Remaining records: {len(taxi_df_clean):,}\")\n",
    "\n",
    "# 2. Remove records with invalid PULocationID or DOLocationID\n",
    "print(\"\\n2. REMOVING RECORDS WITH INVALID LOCATION IDs\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Define valid zone_id range\n",
    "min_zone_id = 1\n",
    "max_zone_id = 263\n",
    "\n",
    "# Create mask for valid location IDs\n",
    "valid_locations_mask = (\n",
    "    (taxi_df_clean['PULocationID'] >= min_zone_id) & \n",
    "    (taxi_df_clean['PULocationID'] <= max_zone_id) &\n",
    "    (taxi_df_clean['DOLocationID'] >= min_zone_id) & \n",
    "    (taxi_df_clean['DOLocationID'] <= max_zone_id) &\n",
    "    (taxi_df_clean['PULocationID'].notna()) &\n",
    "    (taxi_df_clean['DOLocationID'].notna())\n",
    ")\n",
    "\n",
    "invalid_locations_count = len(taxi_df_clean) - valid_locations_mask.sum()\n",
    "print(f\"Records with invalid location IDs: {invalid_locations_count:,}\")\n",
    "\n",
    "taxi_df_clean = taxi_df_clean[valid_locations_mask].copy()\n",
    "cleaning_steps.append(f\"Invalid location IDs: {invalid_locations_count:,}\")\n",
    "print(f\"Remaining records: {len(taxi_df_clean):,}\")\n",
    "\n",
    "# 3. Remove zero distance trips\n",
    "print(\"\\n3. REMOVING ZERO DISTANCE TRIPS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "zero_distance_mask = taxi_df_clean['trip_distance'] > 0\n",
    "zero_distance_count = len(taxi_df_clean) - zero_distance_mask.sum()\n",
    "print(f\"Records with zero distance: {zero_distance_count:,}\")\n",
    "\n",
    "taxi_df_clean = taxi_df_clean[zero_distance_mask].copy()\n",
    "cleaning_steps.append(f\"Zero distance trips: {zero_distance_count:,}\")\n",
    "print(f\"Remaining records: {len(taxi_df_clean):,}\")\n",
    "\n",
    "# 4. Remove trips with negative duration\n",
    "print(\"\\n4. REMOVING TRIPS WITH NEGATIVE DURATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Ensure trip_duration_minutes is calculated\n",
    "if 'trip_duration_minutes' not in taxi_df_clean.columns:\n",
    "    taxi_df_clean['trip_duration_minutes'] = (\n",
    "        taxi_df_clean['tpep_dropoff_datetime'] - taxi_df_clean['tpep_pickup_datetime']\n",
    "    ).dt.total_seconds() / 60\n",
    "\n",
    "negative_duration_mask = taxi_df_clean['trip_duration_minutes'] > 0\n",
    "negative_duration_count = len(taxi_df_clean) - negative_duration_mask.sum()\n",
    "print(f\"Records with negative duration: {negative_duration_count:,}\")\n",
    "\n",
    "taxi_df_clean = taxi_df_clean[negative_duration_mask].copy()\n",
    "cleaning_steps.append(f\"Negative duration trips: {negative_duration_count:,}\")\n",
    "print(f\"Remaining records: {len(taxi_df_clean):,}\")\n",
    "\n",
    "# 5. Remove records with negative fare values\n",
    "print(\"\\n5. REMOVING RECORDS WITH NEGATIVE FARE VALUES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "fare_cols = ['fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'total_amount']\n",
    "negative_fare_mask = pd.Series([True] * len(taxi_df_clean), index=taxi_df_clean.index)\n",
    "\n",
    "for col in fare_cols:\n",
    "    if col in taxi_df_clean.columns:\n",
    "        col_mask = taxi_df_clean[col] >= 0\n",
    "        negative_count = len(taxi_df_clean) - col_mask.sum()\n",
    "        if negative_count > 0:\n",
    "            print(f\"  {col}: {negative_count:,} negative values\")\n",
    "        negative_fare_mask = negative_fare_mask & col_mask\n",
    "\n",
    "total_negative_fare_count = len(taxi_df_clean) - negative_fare_mask.sum()\n",
    "print(f\"Total records with negative fare values: {total_negative_fare_count:,}\")\n",
    "\n",
    "taxi_df_clean = taxi_df_clean[negative_fare_mask].copy()\n",
    "cleaning_steps.append(f\"Negative fare values: {total_negative_fare_count:,}\")\n",
    "print(f\"Remaining records: {len(taxi_df_clean):,}\")\n",
    "\n",
    "# 6. Remove records with missing values in critical columns\n",
    "print(\"\\n6. REMOVING RECORDS WITH MISSING VALUES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Define critical columns that shouldn't have missing values\n",
    "critical_cols = ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'trip_distance', \n",
    "                'fare_amount', 'total_amount', 'PULocationID', 'DOLocationID']\n",
    "\n",
    "missing_mask = pd.Series([True] * len(taxi_df_clean), index=taxi_df_clean.index)\n",
    "\n",
    "for col in critical_cols:\n",
    "    if col in taxi_df_clean.columns:\n",
    "        col_mask = taxi_df_clean[col].notna()\n",
    "        missing_count = len(taxi_df_clean) - col_mask.sum()\n",
    "        if missing_count > 0:\n",
    "            print(f\"  {col}: {missing_count:,} missing values\")\n",
    "        missing_mask = missing_mask & col_mask\n",
    "\n",
    "total_missing_count = len(taxi_df_clean) - missing_mask.sum()\n",
    "print(f\"Total records with missing critical values: {total_missing_count:,}\")\n",
    "\n",
    "taxi_df_clean = taxi_df_clean[missing_mask].copy()\n",
    "cleaning_steps.append(f\"Missing critical values: {total_missing_count:,}\")\n",
    "print(f\"Remaining records: {len(taxi_df_clean):,}\")\n",
    "\n",
    "# 7. Remove trips with 0 passengers\n",
    "print(\"\\n7. REMOVING TRIPS WITH 0 PASSENGERS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if 'passenger_count' in taxi_df_clean.columns:\n",
    "    zero_passenger_mask = taxi_df_clean['passenger_count'] > 0\n",
    "    zero_passenger_count = len(taxi_df_clean) - zero_passenger_mask.sum()\n",
    "    print(f\"Records with 0 passengers: {zero_passenger_count:,}\")\n",
    "    \n",
    "    taxi_df_clean = taxi_df_clean[zero_passenger_mask].copy()\n",
    "    cleaning_steps.append(f\"Zero passengers: {zero_passenger_count:,}\")\n",
    "    print(f\"Remaining records: {len(taxi_df_clean):,}\")\n",
    "else:\n",
    "    print(\"passenger_count column not found, skipping this step\")\n",
    "    cleaning_steps.append(\"Zero passengers: N/A (column not found)\")\n",
    "\n",
    "# 8. Additional data quality checks\n",
    "print(\"\\n8. ADDITIONAL DATA QUALITY CHECKS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Remove extremely long trips (more than 24 hours)\n",
    "long_trip_mask = taxi_df_clean['trip_duration_minutes'] <= (24 * 60)\n",
    "long_trip_count = len(taxi_df_clean) - long_trip_mask.sum()\n",
    "if long_trip_count > 0:\n",
    "    print(f\"Records with trips longer than 24 hours: {long_trip_count:,}\")\n",
    "    taxi_df_clean = taxi_df_clean[long_trip_mask].copy()\n",
    "    cleaning_steps.append(f\"Trips > 24 hours: {long_trip_count:,}\")\n",
    "    print(f\"Remaining records: {len(taxi_df_clean):,}\")\n",
    "\n",
    "# Remove trips with unrealistic distances (more than 100 miles)\n",
    "distance_mask = taxi_df_clean['trip_distance'] <= 100\n",
    "distance_count = len(taxi_df_clean) - distance_mask.sum()\n",
    "if distance_count > 0:\n",
    "    print(f\"Records with trips longer than 100 miles: {distance_count:,}\")\n",
    "    taxi_df_clean = taxi_df_clean[distance_mask].copy()\n",
    "    cleaning_steps.append(f\"Trips > 100 miles: {distance_count:,}\")\n",
    "    print(f\"Remaining records: {len(taxi_df_clean):,}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA CLEANING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"Original dataset: {len(taxi_df):,} records\")\n",
    "print(f\"Final cleaned dataset: {len(taxi_df_clean):,} records\")\n",
    "print(f\"Total records removed: {len(taxi_df) - len(taxi_df_clean):,}\")\n",
    "print(f\"Percentage of data retained: {len(taxi_df_clean)/len(taxi_df)*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nCleaning steps breakdown:\")\n",
    "for i, step in enumerate(cleaning_steps, 1):\n",
    "    print(f\"  {i}. {step}\")\n",
    "\n",
    "# Reset index for clean dataset\n",
    "taxi_df_clean = taxi_df_clean.reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nCleaned dataset info:\")\n",
    "print(f\"Shape: {taxi_df_clean.shape}\")\n",
    "print(f\"Date range: {taxi_df_clean['tpep_pickup_datetime'].min()} to {taxi_df_clean['tpep_pickup_datetime'].max()}\")\n",
    "print(f\"Trip distance range: {taxi_df_clean['trip_distance'].min():.2f} to {taxi_df_clean['trip_distance'].max():.2f} miles\")\n",
    "print(f\"Trip duration range: {taxi_df_clean['trip_duration_minutes'].min():.2f} to {taxi_df_clean['trip_duration_minutes'].max():.2f} minutes\")\n",
    "\n",
    "# Save cleaned dataset\n",
    "save_cleaned = input(\"\\nDo you want to save the cleaned dataset to a file? (y/n): \")\n",
    "if save_cleaned.lower() == 'y':\n",
    "    try:\n",
    "        # Try saving as parquet first\n",
    "        output_file = 'yellow_tripdata_2023-01_cleaned.parquet'\n",
    "        \n",
    "        # Drop any period columns that might cause PyArrow issues\n",
    "        df_to_save = taxi_df_clean.copy()\n",
    "        cols_to_drop = ['pickup_year_month', 'dropoff_year_month']\n",
    "        for col in cols_to_drop:\n",
    "            if col in df_to_save.columns:\n",
    "                df_to_save = df_to_save.drop(columns=[col])\n",
    "                print(f\"Removed column '{col}' for saving\")\n",
    "        \n",
    "        df_to_save.to_parquet(output_file, index=False)\n",
    "        print(f\"Cleaned dataset saved as '{output_file}'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving as parquet: {e}\")\n",
    "        print(\"Trying to save as CSV instead...\")\n",
    "        \n",
    "        # Fallback to CSV\n",
    "        output_file = 'yellow_tripdata_2023-01_cleaned.csv'\n",
    "        taxi_df_clean.to_csv(output_file, index=False)\n",
    "        print(f\"Cleaned dataset saved as '{output_file}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA CLEANING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67b25db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add driver_id and customer_id columns to cleaned dataset\n",
    "print(\"=\"*60)\n",
    "print(\"ADDING DRIVER_ID AND CUSTOMER_ID COLUMNS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Set random seed for reproducibility (optional)\n",
    "np.random.seed(42)  # Remove this line if you want truly random values each time\n",
    "\n",
    "print(f\"Dataset shape before adding columns: {taxi_df_clean.shape}\")\n",
    "\n",
    "# Add driver_id column (randomized from 1 to 80,000)\n",
    "taxi_df_clean['driver_id'] = np.random.randint(1, 80001, size=len(taxi_df_clean))\n",
    "\n",
    "# Add customer_id column (randomized from 1 to 2,000,000)\n",
    "taxi_df_clean['customer_id'] = np.random.randint(1, 2000001, size=len(taxi_df_clean))\n",
    "\n",
    "print(f\"Dataset shape after adding columns: {taxi_df_clean.shape}\")\n",
    "\n",
    "# Verify the new columns\n",
    "print(f\"\\nDriver ID range: {taxi_df_clean['driver_id'].min()} to {taxi_df_clean['driver_id'].max()}\")\n",
    "print(f\"Customer ID range: {taxi_df_clean['customer_id'].min()} to {taxi_df_clean['customer_id'].max()}\")\n",
    "\n",
    "print(f\"\\nUnique driver IDs: {taxi_df_clean['driver_id'].nunique():,}\")\n",
    "print(f\"Unique customer IDs: {taxi_df_clean['customer_id'].nunique():,}\")\n",
    "\n",
    "# Show sample of the new columns\n",
    "print(f\"\\nSample of new columns:\")\n",
    "sample_cols = ['driver_id', 'customer_id', 'tpep_pickup_datetime', 'PULocationID', 'DOLocationID']\n",
    "print(taxi_df_clean[sample_cols].head(10))\n",
    "\n",
    "# Show updated column list\n",
    "print(f\"\\nUpdated columns ({len(taxi_df_clean.columns)} total):\")\n",
    "for i, col in enumerate(taxi_df_clean.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "# Optional: Save updated dataset with new columns\n",
    "save_updated = input(\"\\nDo you want to save the dataset with new driver_id and customer_id columns? (y/n): \")\n",
    "if save_updated.lower() == 'y':\n",
    "    try:\n",
    "        # Try saving as parquet first\n",
    "        output_file = 'yellow_tripdata_2023-01_cleaned_with_ids.parquet'\n",
    "        \n",
    "        # Drop any period columns that might cause PyArrow issues\n",
    "        df_to_save = taxi_df_clean.copy()\n",
    "        cols_to_drop = ['pickup_year_month', 'dropoff_year_month']\n",
    "        for col in cols_to_drop:\n",
    "            if col in df_to_save.columns:\n",
    "                df_to_save = df_to_save.drop(columns=[col])\n",
    "                print(f\"Removed column '{col}' for saving\")\n",
    "        \n",
    "        df_to_save.to_parquet(output_file, index=False)\n",
    "        print(f\"Updated dataset saved as '{output_file}'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving as parquet: {e}\")\n",
    "        print(\"Trying to save as CSV instead...\")\n",
    "        \n",
    "        # Fallback to CSV\n",
    "        output_file = 'yellow_tripdata_2023-01_cleaned_with_ids.csv'\n",
    "        taxi_df_clean.to_csv(output_file, index=False)\n",
    "        print(f\"Updated dataset saved as '{output_file}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DRIVER_ID AND CUSTOMER_ID ADDITION COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68abf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PREPARING DATASET TO MATCH TRIPS TABLE DEFINITION\n",
      "======================================================================\n",
      "Current dataset shape: (2832171, 28)\n",
      "Current columns: ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'airport_fee', 'trip_duration_minutes', 'pickup_year', 'dropoff_year', 'pickup_month', 'dropoff_month', 'pickup_year_month', 'dropoff_year_month', 'driver_id', 'customer_id']\n",
      "\n",
      "--- Column Mapping Analysis ---\n",
      "Required columns: 21\n",
      "Existing columns: 21\n",
      "Missing columns: 0\n",
      "\n",
      "--- Dataset After Filtering ---\n",
      "New shape: (2832171, 22)\n",
      "Columns retained: 21 + trip_id\n",
      "\n",
      "Final columns (22 total):\n",
      "   1. trip_id\n",
      "   2. driver_id\n",
      "   3. customer_id\n",
      "   4. VendorID\n",
      "   5. tpep_pickup_datetime\n",
      "   6. tpep_dropoff_datetime\n",
      "   7. passenger_count\n",
      "   8. trip_distance\n",
      "   9. RatecodeID\n",
      "  10. store_and_fwd_flag\n",
      "  11. PULocationID\n",
      "  12. DOLocationID\n",
      "  13. payment_type\n",
      "  14. fare_amount\n",
      "  15. extra\n",
      "  16. mta_tax\n",
      "  17. tip_amount\n",
      "  18. tolls_amount\n",
      "  19. improvement_surcharge\n",
      "  20. total_amount\n",
      "  21. congestion_surcharge\n",
      "  22. airport_fee\n",
      "\n",
      "Sample of filtered dataset:\n",
      "   trip_id  driver_id  customer_id  VendorID tpep_pickup_datetime  \\\n",
      "0        1      15796       930927         2  2023-01-01 00:32:10   \n",
      "1        2        861       293051         2  2023-01-01 00:55:08   \n",
      "2        3      76821         9973         2  2023-01-01 00:25:04   \n",
      "3        4      54887       829772         2  2023-01-01 00:10:29   \n",
      "4        5       6266      1643547         2  2023-01-01 00:50:34   \n",
      "\n",
      "  tpep_dropoff_datetime  passenger_count  trip_distance  RatecodeID  \\\n",
      "0   2023-01-01 00:40:36              1.0           0.97         1.0   \n",
      "1   2023-01-01 01:01:27              1.0           1.10         1.0   \n",
      "2   2023-01-01 00:37:49              1.0           2.51         1.0   \n",
      "3   2023-01-01 00:21:19              1.0           1.43         1.0   \n",
      "4   2023-01-01 01:02:52              1.0           1.84         1.0   \n",
      "\n",
      "  store_and_fwd_flag  ...  payment_type  fare_amount  extra  mta_tax  \\\n",
      "0                  N  ...             2          9.3    1.0      0.5   \n",
      "1                  N  ...             1          7.9    1.0      0.5   \n",
      "2                  N  ...             1         14.9    1.0      0.5   \n",
      "3                  N  ...             1         11.4    1.0      0.5   \n",
      "4                  N  ...             1         12.8    1.0      0.5   \n",
      "\n",
      "   tip_amount  tolls_amount  improvement_surcharge  total_amount  \\\n",
      "0        0.00           0.0                    1.0         14.30   \n",
      "1        4.00           0.0                    1.0         16.90   \n",
      "2       15.00           0.0                    1.0         34.90   \n",
      "3        3.28           0.0                    1.0         19.68   \n",
      "4       10.00           0.0                    1.0         27.80   \n",
      "\n",
      "   congestion_surcharge  airport_fee  \n",
      "0                   2.5          0.0  \n",
      "1                   2.5          0.0  \n",
      "2                   2.5          0.0  \n",
      "3                   2.5          0.0  \n",
      "4                   2.5          0.0  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "\n",
      "--- Dropped Columns (7 total) ---\n",
      "  - dropoff_month\n",
      "  - dropoff_year\n",
      "  - dropoff_year_month\n",
      "  - pickup_month\n",
      "  - pickup_year\n",
      "  - pickup_year_month\n",
      "  - trip_duration_minutes\n",
      "\n",
      "--- Data Type Verification ---\n",
      "trip_id: int64\n",
      "driver_id: int32\n",
      "customer_id: int32\n",
      "VendorID: int64\n",
      "payment_type: int64\n",
      "\n",
      "--- Final Data Quality Check ---\n",
      "Total records: 2,832,171\n",
      "trip_id range: 1 to 2832171\n",
      "Null values by column:\n",
      "  trip_id: 0\n",
      "  driver_id: 0\n",
      "  customer_id: 0\n",
      "  VendorID: 0\n",
      "  tpep_pickup_datetime: 0\n",
      "  tpep_dropoff_datetime: 0\n",
      "  passenger_count: 0\n",
      "  trip_distance: 0\n",
      "  RatecodeID: 0\n",
      "  store_and_fwd_flag: 0\n",
      "  PULocationID: 0\n",
      "  DOLocationID: 0\n",
      "  payment_type: 0\n",
      "  fare_amount: 0\n",
      "  extra: 0\n",
      "  mta_tax: 0\n",
      "  tip_amount: 0\n",
      "  tolls_amount: 0\n",
      "  improvement_surcharge: 0\n",
      "  total_amount: 0\n",
      "  congestion_surcharge: 0\n",
      "  airport_fee: 0\n",
      "Error saving as parquet: A type extension with name pandas.period already defined\n",
      "Trying to save as CSV instead...\n",
      "Filtered dataset saved as 'yellow-taxi-01-2023-cleaned.csv'\n",
      "\n",
      "======================================================================\n",
      "DATASET PREPARATION COMPLETE\n",
      "======================================================================\n",
      "Final dataset 'trips_df' is ready for database import\n",
      "Records: 2,832,171\n",
      "Columns: 22\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns to match the trips table definition\n",
    "print(\"=\"*70)\n",
    "print(\"PREPARING DATASET TO MATCH TRIPS TABLE DEFINITION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"Current dataset shape: {taxi_df_clean.shape}\")\n",
    "print(f\"Current columns: {list(taxi_df_clean.columns)}\")\n",
    "\n",
    "# Define the columns we need to keep based on the trips table definition\n",
    "required_columns = [\n",
    "    'driver_id',           # already added\n",
    "    'customer_id',         # already added\n",
    "    'VendorID',           # maps to vendorid\n",
    "    'tpep_pickup_datetime',\n",
    "    'tpep_dropoff_datetime', \n",
    "    'passenger_count',\n",
    "    'trip_distance',\n",
    "    'RatecodeID',         # maps to ratecodeid\n",
    "    'store_and_fwd_flag',\n",
    "    'PULocationID',       # maps to pulocationid\n",
    "    'DOLocationID',       # maps to dolocationid\n",
    "    'payment_type',\n",
    "    'fare_amount',\n",
    "    'extra',\n",
    "    'mta_tax',\n",
    "    'tip_amount',\n",
    "    'tolls_amount',\n",
    "    'improvement_surcharge',\n",
    "    'total_amount',\n",
    "    'congestion_surcharge',\n",
    "    'airport_fee'\n",
    "]\n",
    "\n",
    "# Check which required columns exist in the dataset\n",
    "existing_columns = []\n",
    "missing_columns = []\n",
    "\n",
    "for col in required_columns:\n",
    "    if col in taxi_df_clean.columns:\n",
    "        existing_columns.append(col)\n",
    "    else:\n",
    "        missing_columns.append(col)\n",
    "\n",
    "print(f\"\\n--- Column Mapping Analysis ---\")\n",
    "print(f\"Required columns: {len(required_columns)}\")\n",
    "print(f\"Existing columns: {len(existing_columns)}\")\n",
    "print(f\"Missing columns: {len(missing_columns)}\")\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"\\nMissing columns:\")\n",
    "    for col in missing_columns:\n",
    "        print(f\"  - {col}\")\n",
    "\n",
    "# Check for cbd_congestion_fee (it's optional in the table definition)\n",
    "if 'cbd_congestion_fee' in taxi_df_clean.columns:\n",
    "    existing_columns.append('cbd_congestion_fee')\n",
    "    print(f\"\\nOptional column 'cbd_congestion_fee' found and will be included\")\n",
    "\n",
    "# Create the filtered dataset with only required columns\n",
    "taxi_df_filtered = taxi_df_clean[existing_columns].copy()\n",
    "\n",
    "# Add trip_id as auto-incrementing primary key\n",
    "taxi_df_filtered.insert(0, 'trip_id', range(1, len(taxi_df_filtered) + 1))\n",
    "\n",
    "print(f\"\\n--- Dataset After Filtering ---\")\n",
    "print(f\"New shape: {taxi_df_filtered.shape}\")\n",
    "print(f\"Columns retained: {len(taxi_df_filtered.columns) - 1} + trip_id\")  # -1 because trip_id is added\n",
    "\n",
    "# Show the final column list\n",
    "print(f\"\\nFinal columns ({len(taxi_df_filtered.columns)} total):\")\n",
    "for i, col in enumerate(taxi_df_filtered.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "# Show sample of the filtered dataset\n",
    "print(f\"\\nSample of filtered dataset:\")\n",
    "print(taxi_df_filtered.head())\n",
    "\n",
    "# Show columns that were dropped\n",
    "all_original_columns = set(taxi_df_clean.columns)\n",
    "kept_columns = set(existing_columns)\n",
    "dropped_columns = all_original_columns - kept_columns\n",
    "\n",
    "print(f\"\\n--- Dropped Columns ({len(dropped_columns)} total) ---\")\n",
    "for col in sorted(dropped_columns):\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# Data type verification for key columns\n",
    "print(f\"\\n--- Data Type Verification ---\")\n",
    "print(f\"trip_id: {taxi_df_filtered['trip_id'].dtype}\")\n",
    "print(f\"driver_id: {taxi_df_filtered['driver_id'].dtype}\")\n",
    "print(f\"customer_id: {taxi_df_filtered['customer_id'].dtype}\")\n",
    "if 'VendorID' in taxi_df_filtered.columns:\n",
    "    print(f\"VendorID: {taxi_df_filtered['VendorID'].dtype}\")\n",
    "if 'payment_type' in taxi_df_filtered.columns:\n",
    "    print(f\"payment_type: {taxi_df_filtered['payment_type'].dtype}\")\n",
    "\n",
    "# Check for any remaining data quality issues in key columns\n",
    "print(f\"\\n--- Final Data Quality Check ---\")\n",
    "print(f\"Total records: {len(taxi_df_filtered):,}\")\n",
    "print(f\"trip_id range: {taxi_df_filtered['trip_id'].min()} to {taxi_df_filtered['trip_id'].max()}\")\n",
    "print(f\"Null values by column:\")\n",
    "null_counts = taxi_df_filtered.isnull().sum()\n",
    "for col, null_count in null_counts.items():\n",
    "    if null_count > 0:\n",
    "        print(f\"  {col}: {null_count:,}\")\n",
    "    else:\n",
    "        print(f\"  {col}: 0\")\n",
    "\n",
    "# Optional: Save the filtered dataset\n",
    "save_filtered = input(\"\\nDo you want to save the filtered dataset ready for database import? (y/n): \")\n",
    "if save_filtered.lower() == 'y':\n",
    "    try:\n",
    "        # Try saving as parquet first\n",
    "        output_file = 'trips_table_ready.parquet'\n",
    "        taxi_df_filtered.to_parquet(output_file, index=False)\n",
    "        print(f\"Filtered dataset saved as '{output_file}'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving as parquet: {e}\")\n",
    "        print(\"Trying to save as CSV instead...\")\n",
    "        \n",
    "        # Fallback to CSV\n",
    "        output_file = 'yellow-taxi-01-2023-cleaned.csv'\n",
    "        taxi_df_filtered.to_csv(output_file, index=False)\n",
    "        print(f\"Filtered dataset saved as '{output_file}'\")\n",
    "\n",
    "# Store the filtered dataset for further use\n",
    "trips_df = taxi_df_filtered.copy()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATASET PREPARATION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Final dataset 'trips_df' is ready for database import\")\n",
    "print(f\"Records: {len(trips_df):,}\")\n",
    "print(f\"Columns: {len(trips_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726603c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a389b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "REMOVING RECORDS WITH LOCATIONID = 57\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trips_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mREMOVING RECORDS WITH LOCATIONID = 57\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset shape before removal: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtrips_df\u001b[49m.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Check how many records have PULocationID = 57\u001b[39;00m\n\u001b[32m      9\u001b[39m pu_57_count = (trips_df[\u001b[33m'\u001b[39m\u001b[33mPULocationID\u001b[39m\u001b[33m'\u001b[39m] == \u001b[32m57\u001b[39m).sum()\n",
      "\u001b[31mNameError\u001b[39m: name 'trips_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Remove records where PULocationID or DOLocationID is 57\n",
    "print(\"=\"*70)\n",
    "print(\"REMOVING RECORDS WITH LOCATIONID = 57\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"Dataset shape before removal: {trips_df.shape}\")\n",
    "\n",
    "# Check how many records have PULocationID = 57\n",
    "pu_57_count = (trips_df['PULocationID'] == 57).sum()\n",
    "print(f\"Records with PULocationID = 57: {pu_57_count:,}\")\n",
    "\n",
    "# Check how many records have DOLocationID = 57\n",
    "do_57_count = (trips_df['DOLocationID'] == 57).sum()\n",
    "print(f\"Records with DOLocationID = 57: {do_57_count:,}\")\n",
    "\n",
    "# Check how many records have either PULocationID = 57 OR DOLocationID = 57\n",
    "either_57_count = ((trips_df['PULocationID'] == 57) | (trips_df['DOLocationID'] == 57)).sum()\n",
    "print(f\"Records with either PULocationID OR DOLocationID = 57: {either_57_count:,}\")\n",
    "\n",
    "# Create mask to keep records where neither PULocationID nor DOLocationID is 57\n",
    "mask_keep = (trips_df['PULocationID'] != 57) & (trips_df['DOLocationID'] != 57)\n",
    "\n",
    "# Apply the filter\n",
    "trips_df_filtered = trips_df[mask_keep].copy()\n",
    "\n",
    "# Reset index and update trip_id to maintain sequence\n",
    "trips_df_filtered = trips_df_filtered.reset_index(drop=True)\n",
    "trips_df_filtered['trip_id'] = range(1, len(trips_df_filtered) + 1)\n",
    "\n",
    "print(f\"\\nDataset shape after removal: {trips_df_filtered.shape}\")\n",
    "print(f\"Records removed: {len(trips_df) - len(trips_df_filtered):,}\")\n",
    "print(f\"Percentage of data retained: {len(trips_df_filtered)/len(trips_df)*100:.2f}%\")\n",
    "\n",
    "# Verify no records with LocationID = 57 remain\n",
    "remaining_pu_57 = (trips_df_filtered['PULocationID'] == 57).sum()\n",
    "remaining_do_57 = (trips_df_filtered['DOLocationID'] == 57).sum()\n",
    "\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"Remaining records with PULocationID = 57: {remaining_pu_57}\")\n",
    "print(f\"Remaining records with DOLocationID = 57: {remaining_do_57}\")\n",
    "\n",
    "# Update the main dataset\n",
    "trips_df = trips_df_filtered.copy()\n",
    "\n",
    "# Save the updated dataset\n",
    "try:\n",
    "    output_file = 'yellow-taxi-01-2023-cleaned-no-57.csv'\n",
    "    trips_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nUpdated dataset saved as '{output_file}'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error saving file: {e}\")\n",
    "\n",
    "print(f\"\\nFinal dataset info:\")\n",
    "print(f\"Shape: {trips_df.shape}\")\n",
    "print(f\"trip_id range: {trips_df['trip_id'].min()} to {trips_df['trip_id'].max()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"REMOVAL OF LOCATIONID = 57 COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_warehouse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
